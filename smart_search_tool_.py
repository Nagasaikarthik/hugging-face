# -*- coding: utf-8 -*-
"""Smart search Tool .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uavjrq9tNI4NpyEmF-mHphhPJMqVWUZ-
"""

!pip install beautifulsoup4 requests

import requests
from bs4 import BeautifulSoup

url = 'https://courses.analyticsvidhya.com/pages/all-free-courses'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Find all course containers (adjust the tags and classes as needed)
import requests
from bs4 import BeautifulSoup

url = 'https://courses.analyticsvidhya.com/pages/all-free-courses'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# Find all course containers (adjust the tags and classes as needed)
courses = soup.find_all('div', class_='course-card')

course_data = [
    {'title': 'Introduction to Data Science', 'description': 'Learn the basics of Data Science and how to get started'},
    {'title': 'Machine Learning for Beginners', 'description': 'A beginner-friendly introduction to machine learning concepts.'},
    {'title': 'Python for Data Science', 'description': 'An introductory course on using Python for data analysis.'},
    {'title': 'Deep Learning Basics' , 'description': 'Learn the core concepts of deep learning and neural networks.'},
    {'title': 'Introduction to AI and ML','description': 'Covers the fundamentals of AI and machine learning, offering a solid foundation in these domains'},
    {'title': "Getting Started with Neural Networks", 'description': "Explains neural network basics and their applications in machine learning.", 'curriculum': "Introduction to Neural Networks, Activation functions, Backpropagation, Use-cases"},
    {'title': "Ensemble Learning", 'description': "Focuses on boosting and bagging techniques for improving model accuracy.", 'curriculum': "Random Forest, Gradient Boosting, XGBoost, LightGBM"},
    {'title': "Naive Bayes", 'description': "Covers the probabilistic learning approach used for classification tasks.", 'curriculum': "Bayes Theorem, Naive Bayes classifiers, Application in text classification"},
    {'title': "SVM (Support Vector Machines)", 'description': "A course on SVM for classification problems, particularly useful for high-dimensional spaces.", 'curriculum': "SVM theory, Linear and non-linear SVM, Kernels, Practical applications"},
    {'title': "Decision Trees", 'description': "Explains decision trees for both regression and classification tasks.", 'curriculum': "Tree building, Pruning, Entropy, Gini Index, Real-world examples"},
    {'title': "Introduction to NLP", 'description': "An introduction to Natural Language Processing, covering key concepts.", 'curriculum': "Text pre-processing, Tokenization, POS tagging, Sentiment Analysis"},
    {'title': "K-Nearest Neighbors", 'description': "Covers the fundamentals of KNN, a simple yet powerful algorithm.", 'curriculum': "KNN Algorithm, Distance metrics, Use-cases in classification"},
    {'title': "Regression", 'description': "Introduction to linear and logistic regression models.", 'curriculum': "Linear regression, Logistic regression, Model assumptions, Use-cases"},
    {'title': "Introduction to Pandas", 'description': "A hands-on course focused on data analysis using Pandas.", 'curriculum': "DataFrames, Series, Data manipulation, Aggregation, Visualization"},
    {'title': "Sklearn for Machine Learning", 'description': "Introduces the Scikit-learn library for implementing ML algorithms.", 'curriculum': "Installing Sklearn, Model building, Hyperparameter tuning, Evaluation"},
    {'title': "Evaluation Metrics", 'description': "Focuses on evaluating ML models using different metrics.", 'curriculum': "Accuracy, Precision, Recall, F1-score, ROC curves"},
    {'title': "Introduction to Model Deployment", 'description': "A course on deploying machine learning models in production environments.", 'curriculum': "Pickle, Joblib, Flask API, Docker, Heroku"},
    {'title': "Exploratory Data Analysis", 'description': "Explains how to explore data to uncover patterns and insights.", 'curriculum': "Data visualization, Descriptive statistics, Outlier detection, Correlation analysis"},
    {'title': "Introduction to SQL", 'description': "SQL essentials for data science, focusing on querying databases.", 'curriculum': "Basic queries, Joins, Subqueries, Indexing"},
    {'title': "Big Mart Sales Prediction", 'description': "A hands-on project where you predict sales using regression models.", 'curriculum': "Feature engineering, Model building, Evaluation techniques"}

]
for course in courses:
    title = course.find('h3').text
    description = course.find('p').text
    course_data.append({'title': title, 'description': description})

print(course_data)

if not course_data:
    print("No courses found! Check the scraping step.")
else:
    print(f"Found {len(course_data)} courses.")

if isinstance(course_data, list) and isinstance(course_data[0], dict):
    print("Data structure looks good!")
else:
    print("Check course data structure.")

course_texts = []
for course in course_data:
    if isinstance(course, dict):  # Check if the element is a dictionary
        course_texts.append(course.get('title', '') + " " + course.get('description', ''))
    else:
        print(f"Skipping non-dictionary element: {course}")  # Optional: Print a message for skipped elements

print(course_texts)

# After scraping
import numpy as np
print("Course Data:", course_data)

# After generating course texts
print("Course Texts:", course_texts)

# After embeddings are created
try:
    course_embeddings = model.encode(course_texts)
    print("Embeddings created successfully")
except Exception as e:
    print(f"Error creating embeddings: {e}")

# After building the FAISS index
try:
    embeddings = np.array(course_embeddings)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    print("FAISS index built successfully")
except Exception as e:
    print(f"Error building FAISS index: {e}")

!pip install transformers sentence-transformers

from sentence_transformers import SentenceTransformer

# Load a pre-trained model (e.g., 'all-MiniLM-L6-v2')
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Combine titles and descriptions to create course content
# Original code:
# course_texts = [course['title'] + " " + course['description'] for course in course_data]

# Modified code:
course_texts = []  # Initialize an empty list to store course texts
for course in course_data:
    # Check if the element is a dictionary (to handle potential variations in data format)
    if isinstance(course, dict):
        # Access 'title' and 'description' only if they are present in the dictionary
        title = course.get('title', '')
        description = course.get('description', '')
        # Combine title and description and append to the list
        course_texts.append(title + " " + description)
    else:
        # Handle non-dictionary elements by printing a message or taking other appropriate actions
        print(f"Skipping non-dictionary element: {course}")
# Generate embeddings
course_embeddings = model.encode(course_texts)

!pip install faiss-cpu

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Load the pre-trained model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Ensure course_data is not empty and contains the expected data structure
if not course_data or not isinstance(course_data[0], dict) or 'title' not in course_data[0] or 'description' not in course_data[0]:
    print("Error: course_data is empty or does not have the expected format.")
    # Handle the error appropriately, such as raising an exception or exiting the script
    # For this example, we'll use a placeholder
    course_data = [{'title': 'Sample Title', 'description': 'Sample Description'}]

# Combine titles and descriptions
course_texts = [course['title'] + " " + course['description'] for course in course_data]

# Generate embeddings
course_embeddings = model.encode(course_texts)

# Convert embeddings to a NumPy array
embeddings = np.array(course_embeddings)

# Check if embeddings is empty before creating the FAISS index
if embeddings.size == 0:
    print("Error: Embeddings array is empty. Please check the input data.")
else:
    # Build the FAISS index if embeddings is not empty
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    # Example query
    query = "data science basics"
    query_embedding = model.encode([query])

    # Search for similar courses
    D, I = index.search(query_embedding, k=5)  # Get top 5 results
    results = [course_data[i] for i in I[0]]

    for result in results:
        print(result)

!pip install gradio



import gradio as gr

def search_courses(query):
    query_embedding = model.encode([query])
    D, I = index.search(query_embedding, k=5)
    results = [course_data[i]['title'] + " - " + course_data[i]['description'] for i in I[0]]
    return "\n".join(results)

# Create Gradio interface
gr.Interface(fn=search_courses, inputs="text", outputs="text", title="Analytics Vidhya Courses Search").launch()

